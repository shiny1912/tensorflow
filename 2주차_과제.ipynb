{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2주차 과제.ipynb의 사본의 사본",
      "provenance": [],
      "authorship_tag": "ABX9TyPnm3c6aAtyAH8v08Cmuq1F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shiny1912/tensorflow/blob/master/2%EC%A3%BC%EC%B0%A8_%EA%B3%BC%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwG1FH5l0xs8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals, unicode_literals\n",
        "\n",
        "!pip install tf-nightly-2.0-preview\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "[ ] NUM_WORDS = 10000\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=NUM_WORDS)\n",
        "def multi_hot_sequences(sequences, dimension):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, word_indices in enumerate(sequences):\n",
        "      results[i,word_indices] = 1.0\n",
        "    return results\n",
        "\n",
        "train_data = multi_hot_sequences(train_data, dimension=NUM_WORDS)\n",
        "test_data = multi_hot_sequences(test_data, dimension = NUM_WORDS)\n",
        "\n",
        "baseline_model = keras.Sequential([\n",
        "  keras.layers.Dense(16, activation='relu', input_shape=(NUM_WORDS,)),\n",
        "  keras.layers.Dense(16, activation='relu'),\n",
        "  keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "baseline.model.compile(optimizer='adam',\n",
        "                       loss='binary_crossentropy',\n",
        "                       metrics=['accuracy', 'binary_crossentropy'])\n",
        "baseline_model.summary()\n",
        "\n",
        "baseline_history = baseline_model.fit(train_data,\n",
        "                                      train_labels,\n",
        "                                      epochs=20,\n",
        "                                      batch_size=512,\n",
        "                                      validation_data=(test_data,test_labels),\n",
        "                                      verbose=2)\n",
        "#작은모델\n",
        "smaller_model = keras.Sequential([\n",
        "      keras.layers.Dense(4, activation='relu', input_shape=(NUM_WORDS,)),\n",
        "      keras.layers.Dense(4, activation='relu'),\n",
        "      keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "smaller_model_compile(optimizer='adam',\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics='accuracy', 'binary_crossentropy')\n",
        "smaller_model.summary()\n",
        "\n",
        "smaller_history = smaller_model.fit(train_data,\n",
        "                                    train_labels,\n",
        "                                    epochs=20,\n",
        "                                    batch_size=512,\n",
        "                                    validation_data=(test_data,test_labels),\n",
        "                                    verbose=2)\n",
        "bigger_model = keras.models.Sequential([\n",
        "      keras.layers.Dense(512, activation='relu', input_shape=(NUM_WORDS,)),\n",
        "      keras.layers.Dense(512, activation='relu'),\n",
        "      keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "bigger_model.compile(optimizer='adam',\n",
        "                     loss='binary_crossentropy',\n",
        "                     metrics=['accuracy', 'binary_crossentropy'])\n",
        "bigger_model.summary()\n",
        "\n",
        "bigger_history=bigger_model.fit(train_data, train_labels,\n",
        "                                epochs=20,\n",
        "                                batch_size=512,\n",
        "                                validation_data=(test_data, test_labels),\n",
        "                                verbose=2)\n",
        "\n",
        "\n",
        "[ ] def plot_history(histories, key='binary_crossentropy'):\n",
        "      plt.figure(figsize=(16,10))\n",
        "      for name,history in histories:\n",
        "        val = plt.plot(history.epoch, history.history['val_' + key],\n",
        "                       '--', label=name.title()+'Val')\n",
        "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
        "                 label-name.title() + 'Train')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel(key.replace('_', ' ').title())\n",
        "        plt.legend()\n",
        "\n",
        "        plt.xlim([0, max(history.epoch)])\n",
        "\n",
        "      plot_history([('baseline', baseline_history),\n",
        "                    ('smaller', smaller_history),\n",
        "                    ('bigger', bigger_history)]\n",
        "                   )\n",
        "      \n",
        "[  ] l2_model = keras.models.Sequential([\n",
        "         keras.layers.Dense(16, kernel_regularizer=keras.regularizers,l2(0.001),\n",
        "                            activation='relu', input_shape=(NUM_WORDS,)),\n",
        "         keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
        "                            activation='relu'),\n",
        "         keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "l2_model.compile(optimizer='adam',\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['accuracy', 'binary_crossentropy'])\n",
        "l2_model_history = l2.model.fit(train_data, train_labels,\n",
        "                                epochs=20,\n",
        "                                batch_size=512,\n",
        "                                validation_data=(test_data, test_labels)\n",
        "                                verbose=2)\n",
        "plot_history([('baseline', baseline_history),\n",
        "              ('l2', l2_model_history)])\n",
        "dpt_model = keras.models.Sequential([\n",
        "    keras.layers.Dense(16, activation='relu', input_shape=(NUM_WORDS,)),\n",
        "    keras.laers.Dropout(0.5),\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1,activation='sigmoid')\n",
        "])\n",
        "dpt_model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy', 'binary_crossentropy'])\n",
        "dpt_model_history = dpt_model.fit(train_data, train_labels,\n",
        "                                  epochs=20,\n",
        "                                  batch_size=512,\n",
        "                                  validation_data=(test_data, test_labels),\n",
        "                                  verbose=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raQ1qR7M8Ppg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import CSV\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import functools\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\" \n",
        "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
        "\n",
        "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
        "test_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)\n",
        "\n",
        "!head{train_file_path}\n",
        "\n",
        "def get_dataset(file_path, **kwargs) :\n",
        "  dataset = tf.data.experimental.make_csv_dataset(\n",
        "      file_path,\n",
        "      batch_size=5,\n",
        "      label_name=LABEL_COLUMN,\n",
        "      na_value=\"?\",\n",
        "      num_epochs=1,\n",
        "      ignore_errors=True,\n",
        "      **kwargs)\n",
        "  return dataset\n",
        "\n",
        "raw_train_data = get_dataset(train_file_path)\n",
        "raw_test_data = get_dataset(test_file_path)\n",
        "\n",
        "def show_batch(dataset) :\n",
        "  for batch, label in dataset.take()\n",
        "     for key, value in batch.items()\n",
        "       print(\"{:20s}: {}\".format(key, value.numpy()))\n",
        "show_batch(raw_train_data)\n",
        "\n",
        "temp_dataset = get_dataset(train_file_path, column_names = CSV_COLUMNS)\n",
        "show_batch(temp_dataset)\n",
        "temp_dataset = get_dataset(train_file_path, select_columns = SELECT_COLUMNS)\n",
        "show_batch(temp_dataset)\n",
        "\n",
        "temp_dataset = get_dataset(train_file_path,\n",
        "                           select_columns=SELECT_COLUMNS,\n",
        "                           column_defaults = DEFAULTS)\n",
        "show_batch(temp_dataset)\n",
        "\n",
        "example_batch, labels_batch = next(iter(temp_dataset))\n",
        "\n",
        "def pack(features, label) :\n",
        "   return tf.stack(list(features.values()), axis=-1), label\n",
        "packed_dataset = temp_dataset.map(pack)\n",
        "for features, labels in packed_dataset.take(1):\n",
        "  print(features.numpy())\n",
        "  print()\n",
        "  print(labels.numpy())\n",
        "show_batch(raw_train_data)\n",
        "example_batch, labels_batch = next(iter(temp_dataset))\n",
        "\n",
        "class PackNumericFeatures(objcet):\n",
        "  def __init__(self, names):\n",
        "    self.names = names\n",
        "  def __call__(self, features, labels):\n",
        "    numeric_features = [features.pop(name) for name in self.names]\n",
        "    numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features]\n",
        "    numeric_features = tf.stack(numeric_features, axis = -1)\n",
        "    features['numeric'] = numeric_features\n",
        "  return features, labels\n",
        "packed_train_data = raw_train_data.map(\n",
        "    PackNumericFeatures(NUMERIC_FEATURES)\n",
        ")\n",
        "packed_test_data = raw_test_data.map(\n",
        "    PackNumericFeatures(NUMERIC_FEATURES)\n",
        ")\n",
        "import pandas as pd\n",
        "desc = pd.read_csv(train_file_path)[NUMERIC_VALUES].describe()\n",
        "desc\n",
        "MEAN = np.array(desc.T['mean'])\n",
        "STD = np.array(desc.T['std'])\n",
        "def normalize_numeric_data(data, mean, std):\n",
        "  return (data-mean)/std\n",
        "\n",
        "normalizer = functools.partial(normalize_numeric_data, mean=MEAN, std=STD)\n",
        "numeric_column = tf.feature_column.numeric_column('numeric', normalizer_fn = normalizer, shape=[len(NUMERIC_FEAUTRES)]\n",
        "                                                \n",
        ")\n",
        "numeric_columns = [numeric_column]\n",
        "numeric_column\n",
        "example_batch['numeric']\n",
        "numeric_layer = tf.keras.layers.DenseFeatures(numeric_columns)\n",
        "numeric_layer(example.batch).numpy()\n",
        "\n",
        "CATEGORIES = {\n",
        "    \n",
        "}\n",
        "categorical columns = []\n",
        "for feature, vocab in CATEGORIES.items():\n",
        "  cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "      key=feature, vocabulary_list = vocab\n",
        "  )\n",
        "  categorical_columns.append(tf.feature_column.indicator_column(cat_col))\n",
        "  categorical_columns\n",
        "\n",
        "  categorical_layer = tf.keras.layers.DenseFeatures(categorical_columns)\n",
        "  print(categorical_layer(example_batch).numpy()[0])\n",
        "\n",
        "  preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns + numeric_columns)\n",
        "  print(preprocessing_layer(example_batch).numpy()[0])\n",
        "\n",
        "  model = tf.keras.Sequential([\n",
        "     preprocessing_layer,\n",
        "     tf.keras.layers.Dense(128, activation='relu'),\n",
        "     tf.keras.layers.Dense(128, activation='relu'),\n",
        "     tf.keras.Dense(1)\n",
        "  ])\n",
        "  model.compile(\n",
        "      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "      optimizer='adam',\n",
        "      metrics='accuracy'\n",
        "\n",
        "  )\n",
        "  train_data = packed_train_data.shuffle(500)\n",
        "  test_data = packed_test_data\n",
        "\n",
        "  model.fit(train_data, epochs=20)\n",
        "\n",
        "  test_loss, test_accuracy = model.evaluate(test_data)\n",
        "\n",
        "  print(.format(test_loss, test_accuacy))\n",
        "  predictions = model.predict(test_data)\n",
        "  for prediction, survived in zip(predictions[:10], list(test_data)[0][1][:10]):\n",
        "    print(\"\".format(prediction[0]),\n",
        "          \" | Actual outcome :  \",\n",
        "          \"SURVIVED\" if bool(survived) else \"DIED\")\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7QlP9sxGmlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tfrecord\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  !pip install tf-nightly\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import IPython.display as display\n",
        "\n",
        "def _bytes_feature(value):\n",
        "  if isinstance(value, type(tf.constant(0))):\n",
        "    value=value.numpy()\n",
        "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "def _float_feature(value):\n",
        "  return tf.train.Feautre(float_list = tf.train.FloatList(value=[value]))\n",
        "def _int64_feature(value):\n",
        "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "\n",
        "feature = _float_feature(np.exp(1))\n",
        "feature.SerializeToString()\n",
        "n_observations = int(1e4)\n",
        "feature0 = np.random.choice([False, True], n_observations)\n",
        "feature1 = np.random.randint(0,5, n_observations)\n",
        "\n",
        "strings = np.array([b'cat', b'dog', b'chicken', b'horse', b'goat'])\n",
        "feature2 = strings[feature1]\n",
        "\n",
        "feature3 = np.random.randn(n_observations)\n",
        "\n",
        "def serialize_example(feature0, feature1, feature2, feature3) :\n",
        "  feature = {\n",
        "      'feature0' : _int64_feature(feature0),\n",
        "      'feature1' : _int64_feature(feature1),\n",
        "      'feature2' : _bytes_feature(feature2),\n",
        "      'feature3' : _float_feature(feature3)\n",
        "  }\n",
        "example_observation = []\n",
        "serialized_example = serialize_example(False, 4, b'goat', 0.8976)\n",
        "serialized_example\n",
        "\n",
        "example_proto = tf.train.Example.FromString(serialized_example)\n",
        "example_proto\n",
        "\n",
        "uint64 length\n",
        "uint32 masked_crc32_of_length\n",
        "byte data[length]\n",
        "uint32 masked_crc32_of_data\n",
        "\n",
        "masked_crc = ((crc >> 15) | (crc << 17)) + 0xa282ead8ul\n",
        "tf.data.Dataset.from_tensor_slices(feature1)\n",
        "features_dataset = tf.data.Dataset.from_tensor_slices(feature0, feature1, feature2, feature3)\n",
        "features_dataset\n",
        "\n",
        "for f0, f1, f2, f3 in features_dataset.take(1):\n",
        "  print(f0)\n",
        "  print(f1)\n",
        "  print(f2)\n",
        "  print(f3)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yp2Cn-PmI_Mq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#모델 저장과 복원\n",
        "\n",
        "!pip install h5py pyyaml\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals, unicode_literals\n",
        "\n",
        "import os\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "tf.__version__\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "train_labels = train_labels[:1000]\n",
        "test_labels = test_labels[:1000]\n",
        "\n",
        "train_images = train_images[:1000].reshape(-1, 28*28) / 255.0\n",
        "test_images = test_images[:1000], rehsape(01, 28*28) / 255.0\n",
        "\n",
        "def create_model():\n",
        "  model = tf.keras.models.Sequential([\n",
        "      keras.layers.Dense(512, activation='relu', input_shape=784,)),\n",
        "      keras.layers.Dropout(0,2),\n",
        "      keras.layers.Dense(10, activation='softmax')\n",
        "  ])\n",
        "  model.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        " model=create_model()\n",
        " model.summary()\n",
        "\n",
        " checkpoint_path = \"training_1/cp.ckpt\"\n",
        " checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        " cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
        "                                                  save_weights_only=True,\n",
        "                                                  verbose=1)\n",
        " model = create_model()\n",
        " model.fit(train_images, train_labels, epochs=10,\n",
        "           validation_data = (test_images, test_labels),\n",
        "           callbacks = [cp_callback])\n",
        " !ls{checkpoint_dir}\n",
        " model = create_model()\n",
        " loss,acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        " model.load_weights(checkpoint_path)\n",
        " loss.acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        "\n",
        " checkpoint_path = \n",
        " checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        " cp_callback - tf.keras.callbacks.ModelCheckoint(\n",
        "     checkpoint_path, verbose=1, sace_weights_only=True,\n",
        "     period=5\n",
        " )\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}